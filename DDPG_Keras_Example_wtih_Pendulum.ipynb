{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_ALgo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKsAhFzCQbLF",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amifunny/Deep-Learning-Notebook/blob/master/DDPG_Keras_Example_wtih_Pendulum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwVyHryMFvaj",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "**Deep Deterministic Policy Gradient (DDPG)** is a popular algorithm for learning **good actions** corresponding to agent's **State**.\n",
        "\n",
        "This tutorial closely follow this paper -  \n",
        "[Continuous control with deep reinforcement learning](https://arxiv.org/pdf/1509.02971.pdf)\n",
        "\n",
        "# Problem\n",
        "We are trying to solve classic control problem of **Inverted Pendulum**. In this we can take only two actions - Swing LEFT or Swing RIGHT. \n",
        "\n",
        "Now what make this **problem challenging for Q-learning Algorithms** is that **actions are Continuous** instead of being Discrete. That is instead of using two discrete actions like [ -1 or +1] , we have to select from infinite actions ranging from -2 to +2.\n",
        "\n",
        "# Quick Theory\n",
        "\n",
        "Just like A2C Method , we have two Networks -\n",
        "\n",
        "1. Actor - It just takes the action.\n",
        "2. Critic - It tell if action is good( gives +ve value) or   bad(-ve value)\n",
        "\n",
        "But DDPG uses two more tricks -\n",
        "\n",
        "**First, Uses two Target Networks.**\n",
        "\n",
        "**Why?** Because it add stability to training. In short , We are learning from estimated targets and Target Network are updated slowly hence keeping our estimated targets stable.\n",
        "\n",
        "Conceptually it's like saying, \"I have an idea of how to play this well, I'm going to try it out for a bit until I find something better\" as opposed to saying \"I'm going to retrain myself how to play this entire game after every move\". See this answer - [stackoverflow](https://stackoverflow.com/a/54238556/13475679)\n",
        "\n",
        "**Second , Uses Experience Relay.**\n",
        "\n",
        "It is basically list of tuples of (state,action,reward,next_state). So instead of learning from recent experiences , **you learn from sample with fair amount of successful,  failed, early and recent experiences.**\n",
        "\n",
        "\n",
        "Now lets see how is it implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGzWCFUNwakZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use openai gym for Pendulum Env.\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WS3WtOROtpg",
        "colab_type": "text"
      },
      "source": [
        "Standard Way of creating [GYM Environment](http://gym.openai.com/docs). We will use **upper_bound** to scale our actions later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-Lmvw9xyY1z",
        "colab_type": "code",
        "outputId": "fbdea9ff-d0fe-4bdf-a3a6-21e3157f3e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "problem = 'Pendulum-v0'\n",
        "env = gym.make(problem)\n",
        "\n",
        "num_states = env.observation_space.shape[0]\n",
        "print( \"Size of State Space = >  {}\".format(num_states) )\n",
        "num_actions = env.action_space.shape[0]\n",
        "print( \"Size of Action Space = >  {}\".format(num_actions) )\n",
        "\n",
        "upper_bound = env.action_space.high[0]\n",
        "lower_bound = env.action_space.low[0]\n",
        "\n",
        "print( \"Max Value of Action = >  {}\".format(upper_bound) )\n",
        "print( \"Min Value of Action = >  {}\".format(lower_bound) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of State Space = >  3\n",
            "Size of Action Space = >  1\n",
            "Max Value of Action = >  2.0\n",
            "Min Value of Action = >  -2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKT1zvw4PQPJ",
        "colab_type": "text"
      },
      "source": [
        "Now for Exploration by our Actor , we use noisy perturbation, specifically **Ornstein-Uhlenbeck process** as described in paper. Its basically sampling noise from a \"correlated\" normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDI5340tydSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OUActionNoise():\n",
        "  \n",
        "  def __init__(self, mean, std_deviation, theta=.15, dt=1e-2, x0=None):\n",
        "    self.theta = theta\n",
        "    self.mean = mean\n",
        "    self.std_dev = std_deviation\n",
        "    self.dt = dt\n",
        "    self.x0 = x0\n",
        "    self.reset()\n",
        "\n",
        "  def __call__(self):\n",
        "    # Its standard code for this process.\n",
        "    # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "    x = self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \\\n",
        "      + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "    \n",
        "    # This makes this noise more correlated\n",
        "    self.x_prev = x\n",
        "    return x\n",
        "\n",
        "  def reset(self):\n",
        "    if self.x0 is not None:\n",
        "        self.x_prev = self.x0 \n",
        "    else:\n",
        "        self.x_prev = np.zeros_like(self.mean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NC412hrRNSD",
        "colab_type": "text"
      },
      "source": [
        "**Buffer** class implements the Experience Relay Concept.\n",
        "\n",
        "---\n",
        "![Imgur](https://i.imgur.com/mS6iGyJ.jpg)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Critic Loss** - Mean Squared Error of **( y - Q(s,a) )**\n",
        "where **y** is expected return determined by target critic network and Q(s,a) is value of state-action given by critic network. We train our critic network using the computed loss. \n",
        "\n",
        "**y** is a moving target that critic model tries to achieve, but we make this target stable by updating out target model slowly.\n",
        "\n",
        "**Actor Loss** - This is computed using mean of value given by critic model for the actions taken by Actor network. We like to maximize this. So use negative sign before the computed mean and use this to do gradient descent.\n",
        "\n",
        "Hence we update Actor Network such that it produces actions that gets maximum value from critic , for a given state.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQC7oB9ZzSz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Buffer():\n",
        "\n",
        "  def __init__(self,buffer_capacity=100000,batch_size=64):\n",
        "\n",
        "    # Number of \"experiences\" to store at max\n",
        "    self.buffer_capacity = buffer_capacity\n",
        "    # Num of tuples to train on.\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    # Its tells us num of times record() was called.\n",
        "    self.buffer_ctr = 0\n",
        "\n",
        "    # Instead of list of tuples as the exp.relay concept go\n",
        "    # We use different np.arrays for each tuple element\n",
        "    # But is its more easy to convert and keeps thing clean.\n",
        "    self.state_buff = np.zeros( (self.buffer_capacity,num_states) )\n",
        "    self.action_buff = np.zeros( (self.buffer_capacity,num_actions) )\n",
        "    self.reward_buff = np.zeros( (self.buffer_capacity,1) )\n",
        "    self.next_state_buff = np.zeros( (self.buffer_capacity,num_states) )\n",
        "\n",
        "  def record( self , obs_tuple ):\n",
        "\n",
        "    # To make index zero if buffer_capacity excedded\n",
        "    # Hence replacing old records\n",
        "    index = self.buffer_ctr%self.buffer_capacity\n",
        "\n",
        "    self.state_buff[index] = obs_tuple[0]\n",
        "    self.action_buff[index] = obs_tuple[1]\n",
        "    self.reward_buff[index] = obs_tuple[2]\n",
        "    self.next_state_buff[index] = obs_tuple[3]\n",
        "\n",
        "    self.buffer_ctr += 1\n",
        "\n",
        "  # We compute loss and update parameters\n",
        "  def learn(self):\n",
        "\n",
        "    # Get range upto which to sample\n",
        "    record_range = min(self.buffer_ctr, self.buffer_capacity)\n",
        "    # Randomly sample indexes\n",
        "    batch_idx = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "    # Convert to tensors\n",
        "    state_batch = tf.convert_to_tensor( self.state_buff[batch_idx] )\n",
        "    action_batch = tf.convert_to_tensor( self.action_buff[batch_idx] )\n",
        "    reward_batch = tf.convert_to_tensor( self.reward_buff[batch_idx] )\n",
        "    reward_batch = tf.cast( reward_batch , dtype=tf.float32)\n",
        "    next_state_batch = tf.convert_to_tensor( self.next_state_buff[batch_idx] )\n",
        "\n",
        "    # Training and updating Actor - Critic Networks.\n",
        "    # See Pseudo Code.\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        target_actions = target_actor(next_state_batch)\n",
        "        y = reward_batch + GAMMA*target_critic( [next_state_batch,target_actions] )\n",
        "        critic_value = critic_model( [state_batch,action_batch] )\n",
        "        \n",
        "        critic_loss = tf.math.reduce_mean( tf.math.square( y - critic_value ) )\n",
        "\n",
        "    critic_grad = tape.gradient( critic_loss , critic_model.trainable_variables )   \n",
        "    critic_optimizer.apply_gradients(\n",
        "        zip(critic_grad,critic_model.trainable_variables)\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      actions = actor_model(state_batch)\n",
        "      critic_value = critic_model([state_batch,actions])\n",
        "      # Used -ve as we want to max the value given by critic on our actions\n",
        "      actor_loss = -tf.math.reduce_mean( critic_value )\n",
        "\n",
        "    actor_grad = tape.gradient( actor_loss , actor_model.trainable_variables )  \n",
        "    actor_optimizer.apply_gradients(\n",
        "        zip(actor_grad,actor_model.trainable_variables)\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EKc3W0TzrKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This update target Parameters slowly\n",
        "#  On basis of tau that is much less than one.\n",
        "def update_target(tau):\n",
        "\n",
        "  new_weights = []\n",
        "  target_variables = target_critic.weights\n",
        "  for i,variable in enumerate( critic_model.weights ):\n",
        "\n",
        "    new_weights.append( variable*tau + target_variables[i]*(1-tau) )\n",
        "\n",
        "  target_critic.set_weights( new_weights )\n",
        "  \n",
        "  new_weights = []\n",
        "  target_variables = target_actor.weights\n",
        "  for i,variable in enumerate( actor_model.weights ):\n",
        "\n",
        "    new_weights.append( variable*tau + target_variables[i]*(1-tau) )\n",
        "\n",
        "  target_actor.set_weights( new_weights )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbRSBugWV3ca",
        "colab_type": "text"
      },
      "source": [
        "Here we declare Actor and Critic Networks. These are basic Multiple Dense layer Networks with 'ReLU' Activation.\n",
        "\n",
        "NOTICE : We use initialization for last layer of actor to be between -0.003 to 0.003 as this prevents from reaching 1 or -1 value in initial stages which will cut off our Gradient to Zero, as 'tanh' is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi4RxeAu0AdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_actor():\n",
        "\n",
        "  # Initialize weights between -3e-3 and 3-e3\n",
        "  last_init = tf.random_uniform_initializer( minval=-0.003, maxval=0.003 )\n",
        "\n",
        "  inputs = layers.Input( shape=(num_states,) )\n",
        "  out = layers.Dense(512,activation='relu')(inputs)\n",
        "  out = layers.BatchNormalization()(out)\n",
        "  out = layers.Dense(512,activation='relu')(out)\n",
        "  out = layers.BatchNormalization()(out)\n",
        "  outputs = layers.Dense(1,activation='tanh',kernel_initializer=last_init)(out)\n",
        "\n",
        "  # Our upper bound is 2.0 for Pendulum.\n",
        "  # This scale out our Actions\n",
        "  outputs = outputs*upper_bound\n",
        "  model = tf.keras.Model(inputs,outputs)\n",
        "  return model\n",
        "\n",
        "def get_critic():\n",
        "\n",
        "  # State as input\n",
        "  state_input = layers.Input( shape=(num_states) )\n",
        "  state_out = layers.Dense(16, activation='relu')(state_input)\n",
        "  state_out = layers.BatchNormalization()(state_out)\n",
        "  state_out = layers.Dense(32, activation='relu')(state_out)\n",
        "  state_out = layers.BatchNormalization()(state_out)\n",
        "\n",
        "  # Action as input\n",
        "  action_input = layers.Input( shape=(num_actions) )\n",
        "  action_out = layers.Dense(32, activation='relu')(action_input)\n",
        "  action_out = layers.BatchNormalization()(action_out)\n",
        "\n",
        "  # Both are passed through seperate layer before concatenating.\n",
        "  merged = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "  out = layers.Dense(512,activation='relu')(merged)\n",
        "  out = layers.BatchNormalization()(out)\n",
        "  out = layers.Dense(512,activation='relu')(out)\n",
        "  out = layers.BatchNormalization()(out)\n",
        "  outputs = layers.Dense(1)(out)\n",
        "\n",
        "  # Outputs single value for give State-Action \n",
        "  model = tf.keras.Model([state_input,action_input],outputs)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFqqPYuUYJZR",
        "colab_type": "text"
      },
      "source": [
        "Policy( ) returns Action given by our Actor Network plus some Noise for exploration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFo-Yjde0V25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy(state,noise_object):\n",
        "\n",
        "  sampled_actions = tf.squeeze( actor_model(state) )\n",
        "  noise = noise_object()\n",
        "  sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "  # We make sure action is within bounds\n",
        "  legal_action =  np.clip( sampled_actions , lower_bound , upper_bound )\n",
        "\n",
        "  return [np.squeeze(legal_action)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipj3IO7WZfP2",
        "colab_type": "text"
      },
      "source": [
        "HYPER PARAMETERS and OBJECT Declaration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgD1-cPS0dt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stddev = 0.2\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(stddev) * np.ones(1))\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the Weights same at start\n",
        "target_actor.set_weights(actor_model.get_weights()) \n",
        "target_critic.set_weights(critic_model.get_weights()) \n",
        "\n",
        "# learning rate for actor-critic models\n",
        "critic_lr = 0.002\n",
        "actor_lr = 0.001\n",
        "\n",
        "critic_optimizer = tf.keras.optimizers.Adam( critic_lr )\n",
        "actor_optimizer = tf.keras.optimizers.Adam( actor_lr )\n",
        "\n",
        "total_episodes = 100\n",
        "# Discount factor for future rewards\n",
        "GAMMA = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "\n",
        "buffer = Buffer(50000,64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgG6GrJbqbeR",
        "colab_type": "text"
      },
      "source": [
        "Now we implement our Main Loop , and iterate through episodes. We take action using policy() and learn() at each time step, along with updating target networks using 'tau'.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaLxZwF3PNAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ep_reward_list = []\n",
        "avg_reward_list = []\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "\t# Takes about 20 min to train\n",
        "\tfor ep in range(total_episodes):\n",
        "\n",
        "\t\tprev_state = env.reset()\n",
        "\t\tepisodic_r = 0\n",
        "\n",
        "\t\twhile True:\n",
        "\n",
        "\t\t\t# Uncomment this to see the action\n",
        "\t\t\t# But not in notebook.\n",
        "\t\t\t# env.render()\n",
        "\n",
        "\t\t\ttf_prev_state = tf.expand_dims( tf.convert_to_tensor( prev_state ) , 0 )\n",
        "\n",
        "\t\t\taction = policy( tf_prev_state , ou_noise )\n",
        "\t\t\t# Recieve state and reward from environment.\n",
        "\t\t\tstate, reward, done, info = env.step(action)\n",
        "\n",
        "\t\t\tbuffer.record( (prev_state,action,reward,state) )\n",
        "\t\t\tepisodic_r += reward\n",
        "\n",
        "\t\t\tbuffer.learn()\n",
        "\t\t\tupdate_target( tau )\n",
        "\n",
        "\t\t\tif done:\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\t\tprev_state = state\n",
        "\n",
        "\t\tep_reward_list.append( episodic_r )\n",
        "\n",
        "\t\t# Mean of last 40 episodes\n",
        "\t\tavg_reward = np.mean(ep_reward_list[-40:])\n",
        "\t\tprint(\"Episode * {} * Avg Reward is ==> {}\".format(ep,avg_reward))\n",
        "\t\tavg_reward_list.append( avg_reward )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MBOT--W1CNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot a Graph\n",
        "# Episodes vs Avg. Rewards\n",
        "plt.plot( avg_reward_list )\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Avg. Epsiodic Reward') \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAgnRim-8iKt",
        "colab_type": "text"
      },
      "source": [
        "![Graph](https://i.imgur.com/sqEtM6M.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km99B24dc6X6",
        "colab_type": "text"
      },
      "source": [
        "If Networks Learn properly , Average Episodic Reward wil increase with time.\n",
        "\n",
        "Feel Free to Try Different learning rates , tau and architectures for Actor - Critic Networks.\n",
        "\n",
        "The Inverted Pendulum problem has low complexity but DDPG work great on any problem.\n",
        "\n",
        "Another Great Environment to try this on is 'LunarLandingContinuous' but will take more episodes than this but gives good results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGEn-N0PsThx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the Weights\n",
        "actor_model.save_weights('pendulum_actor.h5')\n",
        "critic_model.save_weights('pendulum_critic.h5')\n",
        "\n",
        "target_actor.save_weights('pendulum_t_actor.h5')\n",
        "target_critic.save_weights('pendulum_t_critic.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u1MOECJmzbC",
        "colab_type": "text"
      },
      "source": [
        "Before Training :-\n",
        "\n",
        "![before_img](https://i.imgur.com/ox6b9rC.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hpwisliqmhY",
        "colab_type": "text"
      },
      "source": [
        "After 100 episodes :-\n",
        "\n",
        "![after_img](https://i.imgur.com/eEH8Cz6.gif)\n"
      ]
    }
  ]
}
